{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ARTEMIS Training Notebook (Local)\n",
    "This notebook is a modified version of the original, adapted to run on a local machine.\n",
    "**Dependencies:** Before running, ensure you have installed the required libraries using conda as previously discussed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch_geometric\n",
    "from torch_geometric.data import Data\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "from torch_geometric.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load data from the local ./data directory\n",
    "node_basic_features = torch.load('./data/node_basic_features.pt', weights_only=True).to(device)\n",
    "node_advanced_features = torch.load('./data/node_advanced_features.pt', weights_only=True).to(device)\n",
    "edge_index = torch.load('./data/edge_index.pt', weights_only=True).to(device)\n",
    "base_edge_features = torch.load('./data/base_edge_features.pt', weights_only=True).to(device)\n",
    "nft_multimodal_bmbedding_features = torch.load('./data/nft_multimodal_bmbedding_features.pt', weights_only=True).to(device)\n",
    "y = torch.load('./data/y.pt', weights_only=True).to(device)\n",
    "node_sample_prob = torch.load('./data/node_sample_prob.pt', weights_only=True)\n",
    "node_sample_prob = node_sample_prob / node_sample_prob.sum()\n",
    "\n",
    "node_features = torch.cat([node_basic_features, node_advanced_features], dim=1)\n",
    "edge_features = torch.cat([base_edge_features, nft_multimodal_bmbedding_features], dim=1)\n",
    "\n",
    "train_mask = np.zeros(y.shape[0], dtype=np.bool_)\n",
    "test_mask = np.zeros(y.shape[0], dtype=np.bool_)\n",
    "train_test_split_num = int(y.shape[0] * 0.75)\n",
    "train_index = random.sample(range(y.shape[0]), train_test_split_num)\n",
    "test_index = list(set(range(y.shape[0])) - set(train_index))\n",
    "train_mask[train_index] = True\n",
    "test_mask[test_index] = True\n",
    "print(\"train node num: \", train_mask.sum())\n",
    "print(\"test node num: \", test_mask.sum())\n",
    "print(\"true data percentage in train data: \", y[train_mask].sum() / len(y[train_mask]))\n",
    "print(\"true data percentage in test data: \", y[test_mask].sum() / len(y[test_mask]))\n",
    "\n",
    "data = Data(x=node_features, y=y,\n",
    "            edge_index=edge_index, edge_attr=edge_features,\n",
    "            train_mask=train_mask, test_mask=test_mask).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import SAGEConv, BatchNorm\n",
    "from torch_geometric.nn import MessagePassing\n",
    "\n",
    "class ArtemisFirstLayerConv(MessagePassing):\n",
    "    def __init__(self, in_node_channels, in_edge_channels, out_channels, aggr='mean'):\n",
    "        super(ArtemisFirstLayerConv, self).__init__(aggr=aggr)\n",
    "        self.lin = nn.Linear(in_node_channels + in_edge_channels, out_channels)\n",
    "        self.aggr = aggr\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        return self.propagate(edge_index, size=(x.size(0), x.size(0)), x=x, edge_attr=edge_attr)\n",
    "\n",
    "    def message(self, x_j, edge_attr):\n",
    "        return self.lin(torch.cat([x_j, edge_attr], dim=-1))\n",
    "\n",
    "    def update(self, aggr_out):\n",
    "        return F.relu(aggr_out)\n",
    "\n",
    "class ArtemisNet(nn.Module):\n",
    "    def __init__(self, in_node_channels, in_edge_channels, hidden_channels):\n",
    "        super(ArtemisNet, self).__init__()\n",
    "        self.conv1 = ArtemisFirstLayerConv(in_node_channels, in_edge_channels, hidden_channels)\n",
    "        self.bn1 = BatchNorm(hidden_channels)\n",
    "        self.conv2 = SAGEConv(hidden_channels, hidden_channels)\n",
    "        self.bn2 = BatchNorm(hidden_channels)\n",
    "        self.conv3 = SAGEConv(hidden_channels, hidden_channels)\n",
    "        self.bn3 = BatchNorm(hidden_channels)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(hidden_channels + in_node_channels, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, edge_index_tuple, edge_attr_tuple):\n",
    "        edge_index_0, edge_index_1, edge_index_2 = edge_index_tuple\n",
    "        edge_attr_0, _, _ = edge_attr_tuple\n",
    "\n",
    "        # First layer with residual connection\n",
    "        inital_embedding = x\n",
    "        x = self.conv1(x, edge_index_0, edge_attr_0)\n",
    "        x = F.relu(self.bn1(x))\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "\n",
    "        # Second layer with residual connection\n",
    "        x = self.conv2(x, edge_index_1)\n",
    "        x = F.relu(self.bn2(x))\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "\n",
    "        # Third layer with residual connection\n",
    "        x = self.conv3(x, edge_index_2)\n",
    "        x = F.relu(self.bn3(x))\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "\n",
    "        # Apply MLP to the final output\n",
    "        x = torch.cat([x, inital_embedding], dim=1)\n",
    "        x = self.mlp(x)\n",
    "        return x.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.utils import to_undirected\n",
    "from torch_geometric.sampler import NeighborSampler\n",
    "from typing import List, Optional, Tuple\n",
    "import copy\n",
    "\n",
    "# Compatibility import for different PyG versions\n",
    "try:\n",
    "    from torch_geometric.sparse import SparseTensor\n",
    "except ImportError:\n",
    "    from torch_sparse import SparseTensor\n",
    "\n",
    "# Define a simple container class to replace the removed EdgeIndex\n",
    "class EdgeIndex:\n",
    "    def __init__(self, edge_index, e_id, size):\n",
    "        self.edge_index = edge_index\n",
    "        self.e_id = e_id\n",
    "        self.size = size\n",
    "\n",
    "class NeighborSamplerbyNFT(torch.utils.data.DataLoader):\n",
    "    def __init__(self, edge_index: torch.Tensor, sizes: List[int],\n",
    "                 node_idx: Optional[torch.Tensor] = None, \n",
    "                 edge_attr: Optional[torch.Tensor] = None,\n",
    "                 prob_vector: Optional[torch.Tensor] = None,\n",
    "                 num_nodes: Optional[int] = None,\n",
    "                 return_e_id: bool = True, \n",
    "                 **kwargs):\n",
    "\n",
    "        self.edge_index = edge_index\n",
    "        self.node_idx = node_idx\n",
    "        self.num_nodes = num_nodes\n",
    "        self.sizes = sizes\n",
    "        self.return_e_id = return_e_id\n",
    "        self.edge_attr = edge_attr\n",
    "        self.prob_vector = prob_vector\n",
    "        self.num_nodes = edge_index.max().item() + 1\n",
    "\n",
    "        e_id = torch.arange(edge_index.size(1), device=edge_index.device)\n",
    "        self.adj_t = SparseTensor.from_edge_index(edge_index, e_id, sparse_sizes=(self.num_nodes, self.num_nodes))\n",
    "        self.adj_t = self.adj_t.to(device)\n",
    "\n",
    "    def sample(self, batch):\n",
    "        if not isinstance(batch, torch.Tensor):\n",
    "            batch = torch.tensor(batch)\n",
    "\n",
    "        batch_size: int = len(batch)\n",
    "\n",
    "        adjs = []\n",
    "        n_id = batch\n",
    "        for size in self.sizes:\n",
    "            adj_t, n_id = self.adj_t.sample_adj(n_id, size, replace=False)\n",
    "            row, col, e_id_sampled = adj_t.coo()\n",
    "            edge_index_sampled = torch.stack([row, col], dim=0)\n",
    "            size_sampled = adj_t.sparse_sizes()\n",
    "            adjs.append(EdgeIndex(edge_index=edge_index_sampled, e_id=e_id_sampled, size=size_sampled))\n",
    "\n",
    "        if len(adjs) > 1:\n",
    "            return batch_size, n_id, adjs[::-1]\n",
    "        else:\n",
    "            return batch_size, n_id, adjs[0]\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f'{self.__class__.__name__}(sizes={self.sizes})'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
    "import os\n",
    "\n",
    "log_path = \"./training_log.txt\"\n",
    "\n",
    "if os.path.exists(log_path):\n",
    "    os.remove(log_path)\n",
    "\n",
    "with open(log_path, \"a\") as log_file:\n",
    "    log_file.write(\"Epoch, Average Loss, Average Accuracy, Average Precision, Average Recall, Average F1 Score\\n\")\n",
    "\n",
    "for run in range(5):\n",
    "    print(f\"Starting run {run+1}...\\n\")\n",
    "    with open(log_path, \"a\") as log_file:\n",
    "        model = ArtemisNet(data.x.shape[1], data.edge_attr.shape[1], 32).to(device)\n",
    "\n",
    "        num_pos = data.y[data.train_mask].sum().item()\n",
    "        num_neg = data.train_mask.sum().item() - num_pos\n",
    "        class_weights = torch.tensor([1], dtype=torch.float32).to(device)\n",
    "        criterion = nn.BCEWithLogitsLoss(pos_weight=class_weights).to(device)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, weight_decay=5e-4)\n",
    "\n",
    "        sizes = [8, 1, 1]\n",
    "        edge_sampler = NeighborSamplerbyNFT(edge_index=data.edge_index, sizes=sizes, edge_attr=data.edge_attr, prob_vector=node_sample_prob)\n",
    "        patience = 10\n",
    "        best_loss = float('inf')\n",
    "        patience_counter = 0\n",
    "\n",
    "        train_nodes = torch.where(torch.from_numpy(data.train_mask))[0]\n",
    "\n",
    "        labels = data.y[data.train_mask].cpu().numpy()\n",
    "\n",
    "        class_counts = np.bincount(labels)\n",
    "        weights = 1. / torch.tensor(class_counts, dtype=torch.float32)\n",
    "        sample_weights = weights[labels]\n",
    "\n",
    "        sampler = WeightedRandomSampler(weights=sample_weights, num_samples=len(sample_weights), replacement=False)\n",
    "\n",
    "        best_model = None\n",
    "        best_f1 = 0.0\n",
    "\n",
    "        for epoch in range(100):\n",
    "            model.train()\n",
    "            total_loss = 0\n",
    "            total_accuracy = 0\n",
    "            total_precision = 0\n",
    "            total_recall = 0\n",
    "            total_f1 = 0\n",
    "            batch_count = 0\n",
    "\n",
    "            for subset_nodes in DataLoader(train_nodes, batch_size=256, sampler=sampler):\n",
    "                batch_size, n_id, adjs = edge_sampler.sample(subset_nodes)\n",
    "                n_id = n_id.to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                edge_index_0, e_id_0, size_0 = adjs[0].edge_index, adjs[0].e_id, adjs[0].size\n",
    "                edge_attr_0 = data.edge_attr[e_id_0].to(device)\n",
    "                edge_index_1, _, size_1 = adjs[1].edge_index, adjs[1].e_id, adjs[1].size\n",
    "                edge_index_2, _, size_2 = adjs[2].edge_index, adjs[2].e_id, adjs[2].size\n",
    "\n",
    "                out = model(data.x[n_id], (edge_index_0.to(device), edge_index_1.to(device), edge_index_2.to(device)), (edge_attr_0, None, None))\n",
    "\n",
    "                loss = criterion(out, data.y[n_id].float())\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                total_loss += loss.item()\n",
    "\n",
    "                predictions = torch.sigmoid(out)\n",
    "                pred_binary = (predictions >= 0.5).int()\n",
    "                pred = pred_binary.cpu()\n",
    "                y_true = data.y[n_id].int().cpu()\n",
    "\n",
    "                total_accuracy += accuracy_score(y_true.numpy(), pred.numpy())\n",
    "                total_precision += precision_score(y_true.numpy(), pred.numpy(), zero_division=1)\n",
    "                total_recall += recall_score(y_true.numpy(), pred.numpy())\n",
    "                total_f1 += f1_score(y_true.numpy(), pred.numpy())\n",
    "\n",
    "                batch_count += 1\n",
    "\n",
    "            avg_loss = total_loss / batch_count\n",
    "            avg_accuracy = total_accuracy / batch_count\n",
    "            avg_precision = total_precision / batch_count\n",
    "            avg_recall = total_recall / batch_count\n",
    "            avg_f1 = total_f1 / batch_count\n",
    "\n",
    "            print(f\"Epoch {epoch} | Average Loss: {avg_loss:.5f} | Average Accuracy: {avg_accuracy:.3f} | \"\n",
    "                f\"Average Precision: {avg_precision:.3f} | Average Recall: {avg_recall:.3f} | \"\n",
    "                f\"Average F1 Score: {avg_f1:.3f}\")\n",
    "            log_file.write(f\"{epoch}, {avg_loss:.5f}, {avg_accuracy:.3f}, {avg_precision:.3f}, {avg_recall:.3f}, {avg_f1:.3f}\\n\")\n",
    "\n",
    "            if avg_loss < best_loss:\n",
    "                best_loss = avg_loss\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                if patience_counter == patience:\n",
    "                    print(\"Stopping early due to lack of improvement on the validation set.\")\n",
    "                    break\n",
    "\n",
    "            model.eval()\n",
    "            test_nodes = torch.where(torch.from_numpy(data.test_mask))[0]\n",
    "            with torch.no_grad():\n",
    "                batch_size, n_id, adjs = edge_sampler.sample(test_nodes)\n",
    "                n_id = n_id.to(device)\n",
    "\n",
    "                edge_index_0, e_id_0, size_0 = adjs[0].edge_index, adjs[0].e_id, adjs[0].size\n",
    "                edge_attr_0 = data.edge_attr[e_id_0].to(device)\n",
    "                edge_index_1, _, size_1 = adjs[1].edge_index, adjs[1].e_id, adjs[1].size\n",
    "                edge_index_2, _, size_2 = adjs[2].edge_index, adjs[2].e_id, adjs[2].size\n",
    "\n",
    "                out = model(data.x[n_id], (edge_index_0.to(device), edge_index_1.to(device), edge_index_2.to(device)), (edge_attr_0, None, None))\n",
    "\n",
    "                predictions = torch.sigmoid(out)\n",
    "                pred_binary = (predictions >= 0.5).int()\n",
    "                pred = pred_binary.cpu()\n",
    "                y_true = data.y[n_id].int().cpu()\n",
    "                accuracy = accuracy_score(y_true.numpy(), pred.numpy())\n",
    "                precision = precision_score(y_true.numpy(), pred.numpy(), zero_division=1)\n",
    "                recall = recall_score(y_true.numpy(), pred.numpy())\n",
    "                f1 = f1_score(y_true.numpy(), pred.numpy())\n",
    "\n",
    "                if f1 > best_f1:\n",
    "                    best_f1 = f1\n",
    "                    best_model = model.state_dict().copy()\n",
    "\n",
    "        model.load_state_dict(best_model)\n",
    "        model.eval()\n",
    "        test_nodes = torch.where(torch.from_numpy(data.test_mask))[0]\n",
    "        with torch.no_grad():\n",
    "            batch_size, n_id, adjs = edge_sampler.sample(test_nodes)\n",
    "            n_id = n_id.to(device)\n",
    "\n",
    "            edge_index_0, e_id_0, size_0 = adjs[0].edge_index, adjs[0].e_id, adjs[0].size\n",
    "            edge_attr_0 = data.edge_attr[e_id_0].to.to(device)\n",
    "            edge_index_1, _, size_1 = adjs[1].edge_index, adjs[1].e_id, adjs[1].size\n",
    "            edge_index_2, _, size_2 = adjs[2].edge_index, adjs[2].e_id, adjs[2].size\n",
    "\n",
    "            out = model(data.x[n_id], (edge_index_0.to(device), edge_index_1.to(device), edge_index_2.to(device)), (edge_attr_0, None, None))\n",
    "\n",
    "            predictions = torch.sigmoid(out)\n",
    "            pred_binary = (predictions >= 0.5).int()\n",
    "            pred = pred_binary.cpu()\n",
    "            y_true = data.y[n_id].int().cpu()\n",
    "            accuracy = accuracy_score(y_true.numpy(), pred.numpy())\n",
    "            precision = precision_score(y_true.numpy(), pred.numpy(), zero_division=1)\n",
    "            recall = recall_score(y_true.numpy(), pred.numpy())\n",
    "            f1 = f1_score(y_true.numpy(), pred.numpy())\n",
    "\n",
    "            print(f\"Test - Accuracy: {accuracy:.3f}, Precision: {precision:.3f}, Recall: {recall:.3f}, F1 Score: {f1:.3f}\")\n",
    "            log_file.write(f\"Test - Run {run+1}, Accuracy: {accuracy:.3f}, Precision: {precision:.3f}, Recall: {recall:.3f}, F1 Score: {f1:.3f}\\n\\n\")\n",
    "\n",
    "        print(f\"Run {run+1} completed.\\n\")\n",
    "\n",
    "print(\"All runs completed.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}